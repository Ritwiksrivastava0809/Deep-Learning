{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\RITWIK\n",
      "[nltk_data]     SRIVASTAVA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\RITWIK\n",
      "[nltk_data]     SRIVASTAVA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\RITWIK\n",
      "[nltk_data]     SRIVASTAVA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer # It has the ability to lemmatize.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "\n",
    "nltk.download('punkt')# required package for tokenization\n",
    "nltk.download('wordnet')# word database\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step two: Creating a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"intents\": [\n",
    "\n",
    "            {\"tag\": \"age\",\n",
    "            \"patterns\": [\"how old are you?\"],\n",
    "            \"responses\": [\"I am 2 years old and my birthday was yesterday\"]\n",
    "            },\n",
    "            {\"tag\": \"greeting\",\n",
    "            \"patterns\": [ \"Hi\", \"Hello\", \"Hey\"],\n",
    "            \"responses\": [\"Hi there\", \"Hello\", \"Hi :)\"],\n",
    "            },\n",
    "            {\"tag\": \"goodbye\",\n",
    "            \"patterns\": [ \"bye\", \"later\"],\n",
    "            \"responses\": [\"Bye\", \"take care\"]\n",
    "            },\n",
    "            {\"tag\": \"name\",\n",
    "            \"patterns\": [\"what's your name?\", \"who are you?\"],\n",
    "            \"responses\": [\"I have no name yet,\" \"You can give me one, and I will appreciate it\"]\n",
    "            }\n",
    "\n",
    "]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step three: Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer() #for getting words\n",
    "#list\n",
    "ourclass = []\n",
    "newwords = []\n",
    "documentX = []\n",
    "documentY = []\n",
    "\n",
    "# Each intent is tokenized into words and the patterns and their associated tags are added to their respective lists.\n",
    "\n",
    "for intent in data['intents'] :\n",
    "    for pattern in intent['patterns'] :\n",
    "        ournewtkns = nltk.word_tokenize(pattern)# tokenize the patterns\n",
    "        newwords.extend(ournewtkns)#extends our tokens\n",
    "        documentX.append(pattern)\n",
    "        documentY.append(intent[\"tag\"])\n",
    "        \n",
    "    if intent['tag'] not in ourclass : #add unexisting tag\n",
    "        ourclass.append(intent['tag'])\n",
    "        \n",
    "\n",
    "newwords = [lm.lemmatize(word.lower()) for word in newwords if word not in string.punctuation]\n",
    "newwords = sorted(set(newwords)) #sorting words\n",
    "ourclass = sorted(set(ourclass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step four: Designing a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is used to turn our data into numerical values using bag of words (BoW) encoding system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = [] #trianing list array\n",
    "outempty = [0] * len(ourclass)\n",
    "#  bow model\n",
    "for idx , doc in enumerate(documentX) :\n",
    "    bagofwords = []\n",
    "    text = lm.lemmatize(doc.lower())\n",
    "    for word in newwords :\n",
    "        bagofwords.append(1) if word in text else bagofwords.append(0)\n",
    "        \n",
    "        \n",
    "    outputrow = list(outempty)\n",
    "    outputrow[ourclass.index(documentY[idx])] = 1\n",
    "    trainingData.append([bagofwords , outputrow])\n",
    "\n",
    "random.shuffle(trainingData)\n",
    "trainingData = np.array(trainingData , dtype = object)\n",
    "\n",
    "x = np.array(list(trainingData[:,0])) # first training phase\n",
    "y = np.array(list(trainingData[:,1])) # second training phase  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ishape = (len(x[0]),)\n",
    "oshape = (len(y[0]))\n",
    "\n",
    "#parameter definantion\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128 , input_shape = ishape, activation = \"relu\"))\n",
    "\n",
    "model.add(Dropout(0.5)) # will drop 50% of the neurons to solve overfitting\n",
    "\n",
    "model.add(Dense(64,activation = \"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(oshape,activation = \"softmax\"))\n",
    "\n",
    "md = tf.keras.optimizers.Adam(learning_rate=0.01 , decay = 1e-6)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\",\n",
    "                optimizer = md ,\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x,y,epochs=200,verbose = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step five: Building useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourtext(text) : \n",
    "    newtkns = nltk.word_tokenize(text)\n",
    "    newtkns = [lm.lemmatize(word) for word in newtkns]\n",
    "    return newtkns\n",
    "\n",
    "def wordbag (text,vocab) :\n",
    "    newtkns = ourtext(text)\n",
    "    bagofwords = [0] * len(vocab)\n",
    "    for w in newtkns :\n",
    "        for idx , doc in enumerate(vocab) :\n",
    "            if word == w :\n",
    "                bagofwords[idx] = 1\n",
    "    return np.array(bagofwords)\n",
    "\n",
    "def Pclass (text,vocab,labels) :\n",
    "    bagofwords = wordbag(text,vocab)\n",
    "    ourresult = model.predict(np.array([bagofwords]))[0]\n",
    "    newthresh = 0.2\n",
    "    yp = [[idx,res] for idx , res in enumerate(ourresult)if res > newthresh]\n",
    "    \n",
    "    yp.sort(key = lambda x: x[1],reverse = True)\n",
    "    newlist = []\n",
    "    for r in yp :\n",
    "        newlist.append(labels[r[0]])\n",
    "    return newlist\n",
    "\n",
    "\n",
    "def getres(firstlist , fjson) :\n",
    "    tag =firstlist[0]           \n",
    "    listofintents = fjson['intents']\n",
    "    for i in listofintents :\n",
    "        if i[\"tag\"] == tag :\n",
    "            ourresult = random.choice(i[\"responses\"])\n",
    "            break\n",
    "    return ourresult    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    newMessage = input(\"\")\n",
    "    intents = Pclass(newMessage, newwords, ourclass)\n",
    "    ourResult = getres(intents, data)\n",
    "    print(ourResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7608d97510eb413e289a9ba9d2d6d39358a0b182af5f310d2c58353f8e6821a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
